{"cells":[{"outputs":[{"output_type":"stream","text":"FashionMNIST2065  d2lzh1981\r\n","name":"stdout"}],"execution_count":5,"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/input/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"00C245CDE1834B5A8363966197D471A0","scrolled":false}},{"outputs":[{"output_type":"stream","text":"lost+found\r\n","name":"stdout"}],"execution_count":6,"source":"# 查看个人持久化工作区文件\n!ls /home/kesci/work/","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BD9D8FE368E141258ACC1F2B671A0C0C","scrolled":false}},{"metadata":{"id":"ACF2A93B20AC4D19848D24CEBDB14C47","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 陷入局部最小的解决方案\n\n神经网络有差的局部最小值,深度学习当中,神经网络避免的方法有两个:深度信念网络和Adam算法\n## 深度信念网络(DBN)\n   预训练:寻找一个\n### Hinton的限制波尔兹曼机\n可见单元内部或隐藏单元内部是不相连的\n\n![Image Name](https://cdn.kesci.com/upload/image/q5qxymvlgf.jpg?imageView2/0/w/960/h/960)\n\n* 定义: 限制波尔兹曼机(restricted Boltzmann machine,RBM)是一种可以通过输入数据集学习概率分布的随机生成神经网络. \n* 无向图模型\n\t限制:可见(隐藏)单元没有相互连通\n\t势能函数\n\t$$ \n\tE(v,h)=-\\sum_{i} b_iv_i-\\sum_{j} b_jh_j-\\sum_{i,j} v_iw_{i,j}h_j \n\t$$\n\t$$\n\tp(v,h)=\\frac{1}{Z}e^{-E(v,h)}\n\t$$\n利用多个限制波尔兹曼机构建深度信念网络(DBN),可以实现对观测数据的重构."},{"metadata":{"id":"949E26B66BD349C6A50BB631DEF1BA61","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"F69B18A114674E468EAE21BCCD8F4AA5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## Adam算法--带动量的梯度算法\n带上动量的Adam算法可以帮助越过一些局部最小值,达到更好一些的局部最小值\n* 定义:适应性矩估计(Adam:Adaptive Moment Estimation)\n\t\t\t存储过去时刻梯度平方的指数衰减平均值 $v_t$\n\t\t\t存储过去时刻梯度的指数衰减平均值$m_t$,类似于动量\n\t$$\n\tm_t=\\beta _1 m_{t-1}+(1-\\beta _1)g_t\n\t$$\n\t$$\n\tv_t=\\beta _1 v_{t-1}+(1-\\beta _2)g_t^2\n\t$$\n\t于是有:\n\t$$\n\t\\theta _{t+1}=\\theta_t -\\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\varepsilon}\\hat{m}_t\n\t$$\n\t其中\n\t$$\n\t\\hat{m}_t=\\frac{m_t}{1-\\beta_1^t},\\hat{v}_t=\\frac{v_t}{1-\\beta_2^t}\n\t$$\n\t称为偏置校正估计值\n\t\n![Image Name](https://cdn.kesci.com/upload/image/q5qzdegpkz.jpg?imageView2/0/w/960/h/960)\n"},{"metadata":{"id":"670ED675ED1740C392F5FF444969FB70","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 梯度消失问题的解决方法\n\n## ReLU函数\n去设计更好的激活函数.\n复习梯度消失问题:\n* Sigmoid激活函数:\n$$\n\\sigma(z)=\\frac{1}{1+e^z}\\in(0,1)\n$$\n* 梯度范围可能会变得越来越小\n$$\n$$\n* 在反向传播超过5层后,梯度可能会消失\n激活函数ReLU(rectified linear unit)\n$$\nf_{ReLU}(x)=max(0,x)\n$$\n\n![Image Name](https://cdn.kesci.com/upload/image/q5r0cdmyfd.jpg?imageView2/0/w/960/h/960)\n\n"},{"metadata":{"id":"B85A1334D50D46A7954F0A8539D6DB3B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## ResNet:深度残差网络 \n何凯梅\n提出的背景(很多科学家发现):\n* 训练深度网络很困难\n* 有时即使是在训练数据上更深层次的网络性能也可能比浅层的差(并不是过拟合的问题,而是由于梯度消失的问题)\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5r0m4xc1n.jpg?imageView2/0/w/960/h/960)\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5r0l43tla.jpg?imageView2/0/w/960/h/960)\n"},{"metadata":{"id":"21369C2621AF46F08D006E3C6054268A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"## 批标准化\n有着非常广泛的使用\n对每个输入神经网络的mini-batch在输入之前计算均值和方差,从而标准化,也就是统计学中的计算z分数.\n![Image Name](https://cdn.kesci.com/upload/image/q5r0ore421.jpg?imageView2/0/w/960/h/960)\n\n![Image Name](https://cdn.kesci.com/upload/image/q5r0xteif5.jpg?imageView2/0/w/960/h/960)\n"},{"metadata":{"id":"AAEA98C9F70C410085CFF7C37805ACFE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"mdEditEnable":false},"cell_type":"markdown","source":"# 深度学习中的正则化\n当直接做L1,L2正则化时, 相当于把每一个w都往零的方向拉,但由于深度神经网络是极其敏感的,w的微小的改变可能会把损失函数提升非常高.因此无法直接对深度神经网络直接做L1,L2正则化.\n## Dropout: \n在训练阶段,每个小批量训练时随机禁用网络中一部分节点及其连接.\n\n![Image Name](https://cdn.kesci.com/upload/image/q5r1tpy5as.jpg?imageView2/0/w/960/h/960)\n\n![Image Name](https://cdn.kesci.com/upload/image/q5r1wbhjfr.jpg?imageView2/0/w/960/h/960)\n"},{"metadata":{"id":"E0C40DDAA89741C58DEBA0565A223FA8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}